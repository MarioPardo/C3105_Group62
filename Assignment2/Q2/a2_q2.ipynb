{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from cvxopt import matrix, solvers\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prereq Stuff\n",
    "from scipy.spatial.distance import cdist\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def linearKernel(X1, X2):\n",
    "    return X1 @ X2.T\n",
    "\n",
    "\n",
    "def polyKernel(X1, X2, degree):\n",
    "    return (X1 @ X2.T + 1) ** degree\n",
    "\n",
    "\n",
    "def gaussKernel(X1, X2, width):\n",
    "    distances = cdist(X1, X2, 'sqeuclidean')\n",
    "    return np.exp(- distances / (2*(width**2)))\n",
    "\n",
    "\n",
    "def generateData(n, gen_model):\n",
    "\n",
    "    # Controlling the random seed will give you the same \n",
    "    # random numbers every time you generate the data. \n",
    "    # The seed controls the internal random number generator (RNG).\n",
    "    # Different seeds produce different random numbers. \n",
    "    # This can be handy if you want reproducible results for debugging.\n",
    "    # For example, if your code *sometimes* gives you an error, try\n",
    "    # to find a seed number (0 or others) that produces the error. Then you can\n",
    "    # debug your code step-by-step because every time you get the same data.\n",
    "\n",
    "    # np.random.seed(0)  # control randomness when debugging\n",
    "\n",
    "    if gen_model == 1 or gen_model == 2:\n",
    "        # Gen 1 & 2\n",
    "        d = 2\n",
    "        w_true = np.ones([d, 1])\n",
    "\n",
    "        X = np.random.randn(n, d)\n",
    "\n",
    "        if gen_model == 1:\n",
    "            y = np.sign(X @ w_true)  # generative model 1\n",
    "        else:\n",
    "            y = np.sign((X ** 2) @ w_true - 1)  # generative model 2\n",
    "\n",
    "    elif gen_model == 3:\n",
    "        # Gen 3\n",
    "        X, y = generateMoons(n)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown generative model\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def generateMoons(n, noise=0.1):\n",
    "    n_samples_out = n // 2\n",
    "    n_samples_in = n - n_samples_out\n",
    "    outer_circ_x = np.cos(np.linspace(0, np.pi, n_samples_out))\n",
    "    outer_circ_y = np.sin(np.linspace(0, np.pi, n_samples_out))\n",
    "    inner_circ_x = 1 - np.cos(np.linspace(0, np.pi, n_samples_in))\n",
    "    inner_circ_y = 1 - np.sin(np.linspace(0, np.pi, n_samples_in)) - 0.5\n",
    "\n",
    "    X = np.vstack(\n",
    "        [np.append(outer_circ_x, inner_circ_x), \n",
    "         np.append(outer_circ_y, inner_circ_y)]\n",
    "    ).T\n",
    "    X += np.random.randn(*X.shape) * noise\n",
    "\n",
    "    y = np.hstack(\n",
    "        [-np.ones(n_samples_out, dtype=np.intp), \n",
    "         np.ones(n_samples_in, dtype=np.intp)]\n",
    "    )[:, None]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plotPoints(X, y):\n",
    "    # plot the data points from two classes\n",
    "    X0 = X[y.flatten() >= 0]\n",
    "    X1 = X[y.flatten() < 0]\n",
    "\n",
    "    plt.scatter(X0[:, 0], X0[:, 1], marker='x', label='class -1')\n",
    "    plt.scatter(X1[:, 0], X1[:, 1], marker='o', label='class +1')\n",
    "    return\n",
    "\n",
    "\n",
    "def getRange(X):\n",
    "    x_min = np.amin(X[:, 0]) - 0.1\n",
    "    x_max = np.amax(X[:, 0]) + 0.1\n",
    "    y_min = np.amin(X[:, 1]) - 0.1\n",
    "    y_max = np.amax(X[:, 1]) + 0.1\n",
    "    return x_min, x_max, y_min, y_max\n",
    "\n",
    "\n",
    "def plotModel(X, y, w, w0, classify):\n",
    "\n",
    "    plotPoints(X, y)\n",
    "\n",
    "    # plot model\n",
    "    x_min, x_max, y_min, y_max = getRange(X)\n",
    "    grid_step = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    z = classify(np.c_[xx.ravel(), yy.ravel()], w, w0)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plotAdjModel(X, y, a, a0, kernel_func, adjClassify):\n",
    "\n",
    "    plotPoints(X, y)\n",
    "\n",
    "    # plot model\n",
    "    x_min, x_max, y_min, y_max = getRange(X)\n",
    "    grid_step = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    z = adjClassify(np.c_[xx.ravel(), yy.ravel()], a, a0, X, kernel_func)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plotDualModel(X, y, a, b, lamb, kernel_func, dualClassify):\n",
    "\n",
    "    plotPoints(X, y)\n",
    "\n",
    "    # plot model\n",
    "    x_min, x_max, y_min, y_max = getRange(X)\n",
    "    grid_step = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    z = dualClassify(np.c_[xx.ravel(), yy.ravel()], a, b, X, y, \n",
    "                     lamb, kernel_func)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, z, cmap=plt.cm.RdBu, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def plotDigit(x):\n",
    "    img = x.reshape((28, 28))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [-1.04785175e-03  1.49074224e-04 -3.04967483e-04  9.93145999e-05\n",
      " -5.96727510e-04  5.62978555e-05 -3.82082893e-04 -6.02120793e-04\n",
      "  8.91076438e-05 -6.93995027e-04  3.18651023e-04  2.42809758e-04\n",
      " -2.94464361e-04  6.81757274e-05 -1.57393699e-04  8.55761955e-05\n",
      " -4.56580487e-04  3.11826642e-04  8.33715107e-05  6.64767910e-04\n",
      "  8.24615170e-05 -2.02923487e-06  6.73003799e-05 -3.97935850e-04\n",
      "  8.27187727e-04  6.53168494e-04 -9.24504426e-04 -5.50654988e-04\n",
      " -4.81893521e-04 -5.90709164e-04  2.48423891e-04  3.35830813e-04\n",
      "  2.46673844e-04 -3.04464221e-04 -2.41770337e-04  6.10263941e-04\n",
      " -1.99835619e-04 -1.58542212e-04 -4.95417276e-04  4.71164237e-04\n",
      " -6.09841697e-04 -1.81395168e-04 -4.43910640e-05 -3.34206774e-05\n",
      " -2.09517616e-04  1.59832430e-04 -4.34801857e-04  2.12788646e-04\n",
      " -2.34617382e-04  6.79733796e-04  1.87587418e-04 -1.00804645e-03\n",
      "  3.71343446e-04  3.93347308e-05 -5.32320959e-04 -6.06818097e-04\n",
      " -1.47840895e-04  5.60524734e-04  1.50557957e-04  5.89178707e-04\n",
      " -6.72231297e-04 -6.93429375e-05  1.48386701e-04 -4.36455972e-04\n",
      " -5.78674982e-04  2.72957428e-04  8.24247446e-05 -7.41491335e-04\n",
      "  3.03535647e-04  4.89071153e-04 -2.58570647e-04 -8.74027271e-05\n",
      "  2.91691404e-05  1.68610083e-04 -7.54722109e-04 -1.48284414e-04\n",
      "  3.37128669e-04 -1.11675979e-04  2.09042178e-05 -4.42901071e-04\n",
      " -4.87328176e-04 -6.76946142e-04  1.48081269e-04  9.94821348e-05\n",
      " -9.36464155e-05 -1.31172230e-03  2.12712963e-04  4.65591179e-04\n",
      "  8.01373108e-05  6.13503103e-04  7.66757338e-05 -1.63359483e-04\n",
      "  2.73612985e-04 -3.98156689e-04 -3.56592474e-04 -6.60076155e-04\n",
      " -1.85439213e-04 -5.72141362e-04 -5.75100796e-05  1.87638292e-04]\n",
      "Optimized bias: 0.7537719414788249\n"
     ]
    }
   ],
   "source": [
    "#q2a\n",
    "\n",
    "def objective_function(params, y, lamb, K):\n",
    "    n = len(y)\n",
    "    \n",
    "    alpha = params[:n]\n",
    "    alpha0 = params[n]\n",
    "    \n",
    "    linear_combination = K @ alpha + alpha0\n",
    "    loss = np.sum(np.logaddexp(0, -y * linear_combination))\n",
    "    \n",
    "    regularization = (lamb / 2) * np.dot(alpha.T, K @ alpha)\n",
    "    return loss + regularization\n",
    "\n",
    "\n",
    "def adjBinDev(X, y, lamb, kernel_func):\n",
    "    n, d = X.shape\n",
    "    K = kernel_func(X, X)\n",
    "    initial_params = np.zeros(n + 1)\n",
    "    \n",
    "    result = minimize(objective_function, initial_params, args=(y, lamb, K))\n",
    "    \n",
    "    alpha = result.x[:-1]\n",
    "    alpha0 = result.x[-1]\n",
    "\n",
    "    return alpha, alpha0\n",
    "\n",
    "\n",
    "\n",
    "#  usage\n",
    "X,y = generateData(100,2)\n",
    "lamb = 0.1  \n",
    "kernel_func = lambda X1, X2: np.dot(X1, X2.T)\n",
    "\n",
    "alpha, alpha_0 = adjBinDev(X, y, lamb,kernel_func)\n",
    "\n",
    "print(\"Optimized weights:\", alpha)\n",
    "print(\"Optimized bias:\", alpha_0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "d =  2\n",
      "Shape of y :  (100, 1)\n",
      "Shape of G1: (100, 201)\n",
      "Shape of G2: (100, 201)\n"
     ]
    }
   ],
   "source": [
    "#q2b\n",
    "\n",
    "def adjHinge(X, y, lamb, kernel_func, stabilizer=1e-5):\n",
    "    n, d = X.shape\n",
    "    print(\"n = \", n)\n",
    "    print(\"d = \", d)\n",
    "    K = kernel_func(X, X) \n",
    "\n",
    "    y = np.array(y, dtype=np.double)\n",
    "    print(\"Shape of y : \", y.shape)\n",
    "\n",
    "\n",
    "    P = np.zeros((2*n + 1, 2*n+1)) \n",
    "    P[:n, :n] = K  # Kernel matrix for alpha terms\n",
    "    P = P + stabilizer * np.eye(2*n+1)  # Stabilization\n",
    "\n",
    "    q = np.hstack([np.zeros(n + 1), lamb * np.ones(n)])\n",
    "    \n",
    "    # Create G matrix\n",
    "    # G1: For the non-negativity constraints of the slack variables \n",
    "    G1 = np.zeros((n, n + 1 + n))\n",
    "    print(\"Shape of G1:\", G1.shape)\n",
    "    G1[:, n+1:] = -np.eye(n)  # G13: -Identity matrix for slack variables\n",
    "    \n",
    "    # G2: For the hinge constraints\n",
    "    G2 = np.zeros((n, n + 1 + n))\n",
    "    print(\"Shape of G2:\", G2.shape)\n",
    "    G2[:, :n] = -K @ y[:, np.newaxis].flatten()  # G21: -y * K\n",
    "    G2[:, n] = -y.flatten()              # G22: -y * Identity_n (affecting α_0)\n",
    "    G2[:, n+1:] = -np.eye(n)         # G23: Identity matrix for slack variables\n",
    "\n",
    "    G = np.vstack([G1, G2])  # Stack G1 and G2 to form the full G matrix\n",
    "\n",
    "    # Create the h vector\n",
    "    h = np.hstack([np.zeros(n), -np.ones(n)]) \n",
    "\n",
    "    # Convert \n",
    "    P = matrix(P)\n",
    "    q = matrix(q)\n",
    "    G = matrix(G)\n",
    "    h = matrix(h)\n",
    "  \n",
    "    # Solve the quadratic programming problem\n",
    "    solvers.options['show_progress'] = False\n",
    "    solution = solvers.qp(P, q, G, h)\n",
    "\n",
    "    # Extract solutions for α and α_0\n",
    "    alphas = np.array(solution['x'][:n])\n",
    "    alpha0 = np.array(solution['x'][n])\n",
    "\n",
    "    return alphas, alpha0\n",
    "\n",
    "# Example usage\n",
    "X,y = generateData(100,2)\n",
    "lamb = 1.0  # Regularization parameter\n",
    "\n",
    "kernel_func = lambda X1, X2: np.dot(X1, X2.T)\n",
    "\n",
    "a, a0 = adjHinge(X, y, lamb,kernel_func)\n",
    "\n",
    "#print(\"Optimized weights:\", a.flatten())\n",
    "#print(\"Optimized bias:\", a0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q2c\n",
    "def adjClassify(Xtest, a, a0, X, kernel_func):\n",
    "    yhat = np.sign( (kernel_func(Xtest,X)@ a + a0)  )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Rank(A) < p or Rank([P; A; G]) < n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArithmeticError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/misc.py:1429\u001b[0m, in \u001b[0;36mkkt_chol2.<locals>.factor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(F[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m matrix: \n\u001b[0;32m-> 1429\u001b[0m     lapack\u001b[38;5;241m.\u001b[39mpotrf(F[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArithmeticError\u001b[0m: 80",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mArithmeticError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/coneprog.py:2065\u001b[0m, in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rti \u001b[38;5;129;01min\u001b[39;00m W[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrti\u001b[39m\u001b[38;5;124m'\u001b[39m]: rti[::rti\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m-> 2065\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m kktsolver(W)\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mArithmeticError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/coneprog.py:1981\u001b[0m, in \u001b[0;36mconeqp.<locals>.kktsolver\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mkktsolver\u001b[39m(W):\n\u001b[0;32m-> 1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m factor(W, P)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/misc.py:1444\u001b[0m, in \u001b[0;36mkkt_chol2.<locals>.factor\u001b[0;34m(W, H, Df)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(F[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m matrix: \n\u001b[0;32m-> 1444\u001b[0m     lapack\u001b[38;5;241m.\u001b[39mpotrf(F[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mArithmeticError\u001b[0m: 80",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_train_acc_bindev, avg_test_acc_bindev, avg_train_acc_hinge, avg_test_acc_hinge\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Call function\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m avg_train_acc_bindev, avg_test_acc_bindev, avg_train_acc_hinge, avg_test_acc_hinge \u001b[38;5;241m=\u001b[39m sunExperimentsKernel()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Train Accuracies - Binary Deviance:\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_train_acc_bindev)\n",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m, in \u001b[0;36msunExperimentsKernel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m test_acc_bindev[i, j, r] \u001b[38;5;241m=\u001b[39m compute_accuracy(ytest, test_pred_bindev)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Hinge model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m a, a0 \u001b[38;5;241m=\u001b[39m adjHinge(Xtrain, ytrain, lamb, kernel)\n\u001b[1;32m     44\u001b[0m train_pred_hinge \u001b[38;5;241m=\u001b[39m predict(Xtrain, a, a0, kernel, Xtrain)\n\u001b[1;32m     45\u001b[0m test_pred_hinge \u001b[38;5;241m=\u001b[39m predict(Xtest, a, a0, kernel, Xtrain)\n",
      "Cell \u001b[0;32mIn[15], line 45\u001b[0m, in \u001b[0;36madjHinge\u001b[0;34m(X, y, lamb, kernel_func, stabilizer)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Solve the quadratic programming problem\u001b[39;00m\n\u001b[1;32m     44\u001b[0m solvers\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshow_progress\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m solution \u001b[38;5;241m=\u001b[39m solvers\u001b[38;5;241m.\u001b[39mqp(P, q, G, h)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Extract solutions for α and α_0\u001b[39;00m\n\u001b[1;32m     48\u001b[0m alphas \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(solution[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m][:n])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/coneprog.py:4485\u001b[0m, in \u001b[0;36mqp\u001b[0;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[1;32m   4475\u001b[0m         pinfres, dinfres \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: status, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m: s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m: z,\n\u001b[1;32m   4478\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprimal objective\u001b[39m\u001b[38;5;124m'\u001b[39m: pcost, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdual objective\u001b[39m\u001b[38;5;124m'\u001b[39m: dcost,\n\u001b[1;32m   4479\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgap\u001b[39m\u001b[38;5;124m'\u001b[39m: gap, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelative gap\u001b[39m\u001b[38;5;124m'\u001b[39m: relgap,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4482\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual as primal infeasibility certificate\u001b[39m\u001b[38;5;124m'\u001b[39m: pinfres,\n\u001b[1;32m   4483\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresidual as dual infeasibility certificate\u001b[39m\u001b[38;5;124m'\u001b[39m: dinfres}\n\u001b[0;32m-> 4485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coneqp(P, q, G, h, \u001b[38;5;28;01mNone\u001b[39;00m, A,  b, initvals, kktsolver \u001b[38;5;241m=\u001b[39m kktsolver, options \u001b[38;5;241m=\u001b[39m options)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/cvxopt/coneprog.py:2067\u001b[0m, in \u001b[0;36mconeqp\u001b[0;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: f \u001b[38;5;241m=\u001b[39m kktsolver(W)\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mArithmeticError\u001b[39;00m:\n\u001b[0;32m-> 2067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank(A) < p or Rank([P; A; G]) < n\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;66;03m# Solve\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;66;03m#     [ P   A'  G' ]   [ x ]   [ -q ]\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;66;03m#     [ A   0   0  ] * [ y ] = [  b ].\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;66;03m#     [ G   0  -I  ]   [ z ]   [  h ]\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m xcopy(q, x)\n",
      "\u001b[0;31mValueError\u001b[0m: Rank(A) < p or Rank([P; A; G]) < n"
     ]
    }
   ],
   "source": [
    "#q2d\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def predict(X, alphas, alpha0, kernel, X_support):\n",
    "    K = kernel(X_support, X)\n",
    "    predictions = K.T.dot(alphas) + alpha0\n",
    "    return np.sign(predictions)\n",
    "\n",
    "def sunExperimentsKernel():\n",
    "    n_runs = 10 \n",
    "    n_train = 100\n",
    "    n_test = 1000\n",
    "    lamb = 0.001\n",
    "    kernel_list = [linearKernel,\n",
    "                   lambda X1, X2: polyKernel(X1, X2, 2),\n",
    "                   lambda X1, X2: polyKernel(X1, X2, 3),\n",
    "                   lambda X1, X2: gaussKernel(X1, X2, 1.0),\n",
    "                   lambda X1, X2: gaussKernel(X1, X2, 0.5)]\n",
    "    \n",
    "    gen_model_list = [1, 2, 3]\n",
    "    \n",
    "    train_acc_bindev = np.zeros([len(kernel_list), len(gen_model_list), n_runs])\n",
    "    test_acc_bindev = np.zeros([len(kernel_list), len(gen_model_list), n_runs])\n",
    "    train_acc_hinge = np.zeros([len(kernel_list), len(gen_model_list), n_runs])\n",
    "    test_acc_hinge = np.zeros([len(kernel_list), len(gen_model_list), n_runs])\n",
    "    \n",
    "    for r in range(n_runs):\n",
    "        for i, kernel in enumerate(kernel_list):\n",
    "            for j, gen_model in enumerate(gen_model_list):\n",
    "                Xtrain, ytrain = generateData(n=n_train, gen_model=gen_model)\n",
    "                Xtest, ytest = generateData(n=n_test, gen_model=gen_model)\n",
    "                \n",
    "                # BinDev model\n",
    "                a, a0 = adjBinDev(Xtrain, ytrain, lamb, kernel)\n",
    "                train_pred_bindev = predict(Xtrain, a, a0, kernel, Xtrain)\n",
    "                test_pred_bindev = predict(Xtest, a, a0, kernel, Xtrain)\n",
    "                train_acc_bindev[i, j, r] = compute_accuracy(ytrain, train_pred_bindev)\n",
    "                test_acc_bindev[i, j, r] = compute_accuracy(ytest, test_pred_bindev)\n",
    "                \n",
    "                # Hinge model\n",
    "                a, a0 = adjHinge(Xtrain, ytrain, lamb, kernel)\n",
    "                train_pred_hinge = predict(Xtrain, a, a0, kernel, Xtrain)\n",
    "                test_pred_hinge = predict(Xtest, a, a0, kernel, Xtrain)\n",
    "                train_acc_hinge[i, j, r] = compute_accuracy(ytrain, train_pred_hinge)\n",
    "                test_acc_hinge[i, j, r] = compute_accuracy(ytest, test_pred_hinge)\n",
    "\n",
    "    # Compute average accuracies over runs\n",
    "    avg_train_acc_bindev = np.mean(train_acc_bindev, axis=2)\n",
    "    avg_test_acc_bindev = np.mean(test_acc_bindev, axis=2)\n",
    "    avg_train_acc_hinge = np.mean(train_acc_hinge, axis=2)\n",
    "    avg_test_acc_hinge = np.mean(test_acc_hinge, axis=2)\n",
    "    \n",
    "    # Return or print results\n",
    "    return avg_train_acc_bindev, avg_test_acc_bindev, avg_train_acc_hinge, avg_test_acc_hinge\n",
    "\n",
    "# Call function\n",
    "avg_train_acc_bindev, avg_test_acc_bindev, avg_train_acc_hinge, avg_test_acc_hinge = sunExperimentsKernel()\n",
    "\n",
    "# Print results\n",
    "print(\"Average Train Accuracies - Binary Deviance:\", avg_train_acc_bindev)\n",
    "print(\"Average Test Accuracies - Binary Deviance:\", avg_test_acc_bindev)\n",
    "print(\"Average Train Accuracies - Hinge:\", avg_train_acc_hinge)\n",
    "print(\"Average Test Accuracies - Hinge:\", avg_test_acc_hinge)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
