{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Q2: Gradient Descent and Logistic Regression </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autograd.numpy as np  # when testing gradient\n",
    "from cvxopt import matrix, solvers\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2a)\n",
    "\n",
    "\n",
    "def linearRegL2Obj(w, X, y):\n",
    "\n",
    "    pred = X @ w\n",
    "    diff = pred = y\n",
    "    objectiveVal = (1/2) * np.mean(diff ** 2)\n",
    "    n = X.shape[0]\n",
    "    gradient = (X.T @ diff) / n\n",
    "\n",
    "    return objectiveVal, gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2b\n",
    "\n",
    "def gd(func, w_init, X, y, step_size, max_iter, tol=1e-10):\n",
    "\n",
    "    w = w_init\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        objval, gradient = func(w,X,y)\n",
    "        \n",
    "        norm = np.linalg.norm(gradient)\n",
    "        if norm < tol: #stop when gradient is good enough\n",
    "            break;\n",
    "\n",
    "        w = w - step_size * gradient\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2c\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "def logisticRegObj(w, X, y):\n",
    "    pred = X @ w\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    objval = (1/n) * np.sum(np.logaddexp(0, -pred) - y * pred)\n",
    "    grad = (1/n) * X.T @ (sigmoid(pred) - y)\n",
    "    \n",
    "    return objval, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.9465  , 0.8416  , 0.922   ],\n",
       "        [0.9264  , 0.9218  , 0.92435 ],\n",
       "        [0.9261  , 0.9811  , 0.9315  ],\n",
       "        [0.919425, 0.9994  , 0.90155 ]]),\n",
       " array([[0.903235, 0.840325, 0.91979 ],\n",
       "        [0.91746 , 0.919805, 0.919335],\n",
       "        [0.9196  , 0.97506 , 0.91998 ],\n",
       "        [0.920155, 0.995805, 0.896885]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2d\n",
    "def synClsExperiments():\n",
    "\n",
    "    def genData(n_points, d):\n",
    "        '''\n",
    "        This function generate synthetic data\n",
    "        '''\n",
    "        c0 = np.ones([1, d])  # class 0 center\n",
    "        c1 = -np.ones([1, d])  # class 1 center\n",
    "        X0 = np.random.randn(n_points, d) + c0  # class 0 input\n",
    "        X1 = np.random.randn(n_points, d) + c1  # class 1 input\n",
    "        X = np.concatenate((X0, X1), axis=0)\n",
    "        X = np.concatenate((np.ones((2 * n_points, 1)), X), axis=1)  # augmentation\n",
    "        y = np.concatenate([np.zeros([n_points, 1]), np.ones([n_points, 1])], axis=0)\n",
    "        return X, y\n",
    "\n",
    "    def runClsExp(m=100, d=2, eta=0.1, max_iter=1000, tol=1e-10):\n",
    "        '''\n",
    "        Run classification experiment with the specified arguments\n",
    "        '''\n",
    "\n",
    "        Xtrain, ytrain = genData(m, d)\n",
    "        n_test = 1000\n",
    "        Xtest, ytest = genData(n_test, d)\n",
    "\n",
    "        w_init = np.random.randn(d + 1, 1)\n",
    "        w_logit = gd(logisticRegObj, w_init, Xtrain, ytrain, eta, max_iter, tol)\n",
    "        ytrain_hat = 0.5 * (1 + np.sign(Xtrain @ w_logit))\n",
    "        train_acc = np.sum(ytrain == ytrain_hat) / (2 * m)\n",
    "\n",
    "        ytest_hat = 0.5 * (1 + np.sign(Xtest @ w_logit))\n",
    "        test_acc = np.sum(ytest == ytest_hat) / (2 * n_test)\n",
    "\n",
    "        return train_acc, test_acc\n",
    "\n",
    "    n_runs = 100\n",
    "    train_acc = np.zeros([n_runs, 4, 3])\n",
    "    test_acc = np.zeros([n_runs, 4, 3])\n",
    "    for r in range(n_runs):\n",
    "        for i, m in enumerate((10, 50, 100, 200)):\n",
    "            train_acc[r, i, 0], test_acc[r, i, 0] = runClsExp(m=m)\n",
    "        for i, d in enumerate((1, 2, 4, 8)):\n",
    "            train_acc[r, i, 1], test_acc[r, i, 1] = runClsExp(d=d)\n",
    "        for i, eta in enumerate((0.1, 1.0, 10., 100.)):\n",
    "            train_acc[r, i, 2], test_acc[r, i, 2] = runClsExp(eta=eta)\n",
    "\n",
    "    return np.mean(train_acc, axis=0), np.mean(test_acc, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "synClsExperiments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
